"""
SAMP-Net Composition Scorer Module

Implements Saliency-Augmented Multi-pattern Pooling (SAMP) for image composition assessment.
Based on: https://github.com/bcmi/Image-Composition-Assessment-Dataset-CADB/tree/main/SAMPNet

Model predicts composition quality (1-5 scale) and identifies dominant composition patterns.
Requires saliency maps as input, generated by U2-Net-P.
"""

import os
import numpy as np
import urllib.request

import torch
import torch.nn as nn
import torch.nn.functional as F
import torchvision.transforms as transforms
import torchvision.models as models
from PIL import Image

# 8 composition patterns based on spatial pooling strategies
COMPOSITION_PATTERNS = [
    'global',           # 0: Global average pooling
    'horizontal',       # 1: Upper/lower halves
    'vertical',         # 2: Left/right halves
    'triangular',       # 3: Triangular regions
    'surround',         # 4: Center vs surroundings
    'quarter',          # 5: 2x2 grid
    'cross',            # 6: Cross divisions
    'rule_of_thirds',   # 7: 3x3 composition grid
]

# Weight download URLs
# Note: U2-Net-P weights must be downloaded from Google Drive
U2NETP_GDRIVE_ID = "1rbSTGKAE-MTxBYHd-51l2hMOQPT_7EPy"
U2NETP_GDRIVE_URL = f"https://drive.google.com/uc?export=download&id={U2NETP_GDRIVE_ID}"
SAMPNET_WEIGHTS_URL = "https://www.dropbox.com/scl/fi/k1yuyhotuk9ky3m41iobg/samp_net.pth?rlkey=aoqqxv27wd5qqj3pytxki6vi3&dl=1"


# =============================================================================
# U2-Net-P for Saliency Detection (Lightweight, 4.7MB)
# =============================================================================

class REBNCONV(nn.Module):
    """Basic convolution block with BatchNorm and ReLU."""
    def __init__(self, in_ch=3, out_ch=3, dirate=1):
        super(REBNCONV, self).__init__()
        self.conv_s1 = nn.Conv2d(in_ch, out_ch, 3, padding=1 * dirate, dilation=1 * dirate)
        self.bn_s1 = nn.BatchNorm2d(out_ch)
        self.relu_s1 = nn.ReLU(inplace=True)

    def forward(self, x):
        return self.relu_s1(self.bn_s1(self.conv_s1(x)))


def _upsample_like(src, tar):
    """Upsample src to match tar's spatial dimensions."""
    return F.interpolate(src, size=tar.shape[2:], mode='bilinear', align_corners=False)


class RSU7(nn.Module):
    """RSU-7 block for U2-Net."""
    def __init__(self, in_ch=3, mid_ch=12, out_ch=3):
        super(RSU7, self).__init__()
        self.rebnconvin = REBNCONV(in_ch, out_ch, dirate=1)
        self.rebnconv1 = REBNCONV(out_ch, mid_ch, dirate=1)
        self.pool1 = nn.MaxPool2d(2, stride=2, ceil_mode=True)
        self.rebnconv2 = REBNCONV(mid_ch, mid_ch, dirate=1)
        self.pool2 = nn.MaxPool2d(2, stride=2, ceil_mode=True)
        self.rebnconv3 = REBNCONV(mid_ch, mid_ch, dirate=1)
        self.pool3 = nn.MaxPool2d(2, stride=2, ceil_mode=True)
        self.rebnconv4 = REBNCONV(mid_ch, mid_ch, dirate=1)
        self.pool4 = nn.MaxPool2d(2, stride=2, ceil_mode=True)
        self.rebnconv5 = REBNCONV(mid_ch, mid_ch, dirate=1)
        self.pool5 = nn.MaxPool2d(2, stride=2, ceil_mode=True)
        self.rebnconv6 = REBNCONV(mid_ch, mid_ch, dirate=1)
        self.rebnconv7 = REBNCONV(mid_ch, mid_ch, dirate=2)
        self.rebnconv6d = REBNCONV(mid_ch * 2, mid_ch, dirate=1)
        self.rebnconv5d = REBNCONV(mid_ch * 2, mid_ch, dirate=1)
        self.rebnconv4d = REBNCONV(mid_ch * 2, mid_ch, dirate=1)
        self.rebnconv3d = REBNCONV(mid_ch * 2, mid_ch, dirate=1)
        self.rebnconv2d = REBNCONV(mid_ch * 2, mid_ch, dirate=1)
        self.rebnconv1d = REBNCONV(mid_ch * 2, out_ch, dirate=1)

    def forward(self, x):
        hx = x
        hxin = self.rebnconvin(hx)
        hx1 = self.rebnconv1(hxin)
        hx = self.pool1(hx1)
        hx2 = self.rebnconv2(hx)
        hx = self.pool2(hx2)
        hx3 = self.rebnconv3(hx)
        hx = self.pool3(hx3)
        hx4 = self.rebnconv4(hx)
        hx = self.pool4(hx4)
        hx5 = self.rebnconv5(hx)
        hx = self.pool5(hx5)
        hx6 = self.rebnconv6(hx)
        hx7 = self.rebnconv7(hx6)
        hx6d = self.rebnconv6d(torch.cat((hx7, hx6), 1))
        hx6dup = _upsample_like(hx6d, hx5)
        hx5d = self.rebnconv5d(torch.cat((hx6dup, hx5), 1))
        hx5dup = _upsample_like(hx5d, hx4)
        hx4d = self.rebnconv4d(torch.cat((hx5dup, hx4), 1))
        hx4dup = _upsample_like(hx4d, hx3)
        hx3d = self.rebnconv3d(torch.cat((hx4dup, hx3), 1))
        hx3dup = _upsample_like(hx3d, hx2)
        hx2d = self.rebnconv2d(torch.cat((hx3dup, hx2), 1))
        hx2dup = _upsample_like(hx2d, hx1)
        hx1d = self.rebnconv1d(torch.cat((hx2dup, hx1), 1))
        return hx1d + hxin


class RSU6(nn.Module):
    """RSU-6 block for U2-Net."""
    def __init__(self, in_ch=3, mid_ch=12, out_ch=3):
        super(RSU6, self).__init__()
        self.rebnconvin = REBNCONV(in_ch, out_ch, dirate=1)
        self.rebnconv1 = REBNCONV(out_ch, mid_ch, dirate=1)
        self.pool1 = nn.MaxPool2d(2, stride=2, ceil_mode=True)
        self.rebnconv2 = REBNCONV(mid_ch, mid_ch, dirate=1)
        self.pool2 = nn.MaxPool2d(2, stride=2, ceil_mode=True)
        self.rebnconv3 = REBNCONV(mid_ch, mid_ch, dirate=1)
        self.pool3 = nn.MaxPool2d(2, stride=2, ceil_mode=True)
        self.rebnconv4 = REBNCONV(mid_ch, mid_ch, dirate=1)
        self.pool4 = nn.MaxPool2d(2, stride=2, ceil_mode=True)
        self.rebnconv5 = REBNCONV(mid_ch, mid_ch, dirate=1)
        self.rebnconv6 = REBNCONV(mid_ch, mid_ch, dirate=2)
        self.rebnconv5d = REBNCONV(mid_ch * 2, mid_ch, dirate=1)
        self.rebnconv4d = REBNCONV(mid_ch * 2, mid_ch, dirate=1)
        self.rebnconv3d = REBNCONV(mid_ch * 2, mid_ch, dirate=1)
        self.rebnconv2d = REBNCONV(mid_ch * 2, mid_ch, dirate=1)
        self.rebnconv1d = REBNCONV(mid_ch * 2, out_ch, dirate=1)

    def forward(self, x):
        hx = x
        hxin = self.rebnconvin(hx)
        hx1 = self.rebnconv1(hxin)
        hx = self.pool1(hx1)
        hx2 = self.rebnconv2(hx)
        hx = self.pool2(hx2)
        hx3 = self.rebnconv3(hx)
        hx = self.pool3(hx3)
        hx4 = self.rebnconv4(hx)
        hx = self.pool4(hx4)
        hx5 = self.rebnconv5(hx)
        hx6 = self.rebnconv6(hx5)
        hx5d = self.rebnconv5d(torch.cat((hx6, hx5), 1))
        hx5dup = _upsample_like(hx5d, hx4)
        hx4d = self.rebnconv4d(torch.cat((hx5dup, hx4), 1))
        hx4dup = _upsample_like(hx4d, hx3)
        hx3d = self.rebnconv3d(torch.cat((hx4dup, hx3), 1))
        hx3dup = _upsample_like(hx3d, hx2)
        hx2d = self.rebnconv2d(torch.cat((hx3dup, hx2), 1))
        hx2dup = _upsample_like(hx2d, hx1)
        hx1d = self.rebnconv1d(torch.cat((hx2dup, hx1), 1))
        return hx1d + hxin


class RSU5(nn.Module):
    """RSU-5 block for U2-Net."""
    def __init__(self, in_ch=3, mid_ch=12, out_ch=3):
        super(RSU5, self).__init__()
        self.rebnconvin = REBNCONV(in_ch, out_ch, dirate=1)
        self.rebnconv1 = REBNCONV(out_ch, mid_ch, dirate=1)
        self.pool1 = nn.MaxPool2d(2, stride=2, ceil_mode=True)
        self.rebnconv2 = REBNCONV(mid_ch, mid_ch, dirate=1)
        self.pool2 = nn.MaxPool2d(2, stride=2, ceil_mode=True)
        self.rebnconv3 = REBNCONV(mid_ch, mid_ch, dirate=1)
        self.pool3 = nn.MaxPool2d(2, stride=2, ceil_mode=True)
        self.rebnconv4 = REBNCONV(mid_ch, mid_ch, dirate=1)
        self.rebnconv5 = REBNCONV(mid_ch, mid_ch, dirate=2)
        self.rebnconv4d = REBNCONV(mid_ch * 2, mid_ch, dirate=1)
        self.rebnconv3d = REBNCONV(mid_ch * 2, mid_ch, dirate=1)
        self.rebnconv2d = REBNCONV(mid_ch * 2, mid_ch, dirate=1)
        self.rebnconv1d = REBNCONV(mid_ch * 2, out_ch, dirate=1)

    def forward(self, x):
        hx = x
        hxin = self.rebnconvin(hx)
        hx1 = self.rebnconv1(hxin)
        hx = self.pool1(hx1)
        hx2 = self.rebnconv2(hx)
        hx = self.pool2(hx2)
        hx3 = self.rebnconv3(hx)
        hx = self.pool3(hx3)
        hx4 = self.rebnconv4(hx)
        hx5 = self.rebnconv5(hx4)
        hx4d = self.rebnconv4d(torch.cat((hx5, hx4), 1))
        hx4dup = _upsample_like(hx4d, hx3)
        hx3d = self.rebnconv3d(torch.cat((hx4dup, hx3), 1))
        hx3dup = _upsample_like(hx3d, hx2)
        hx2d = self.rebnconv2d(torch.cat((hx3dup, hx2), 1))
        hx2dup = _upsample_like(hx2d, hx1)
        hx1d = self.rebnconv1d(torch.cat((hx2dup, hx1), 1))
        return hx1d + hxin


class RSU4(nn.Module):
    """RSU-4 block for U2-Net."""
    def __init__(self, in_ch=3, mid_ch=12, out_ch=3):
        super(RSU4, self).__init__()
        self.rebnconvin = REBNCONV(in_ch, out_ch, dirate=1)
        self.rebnconv1 = REBNCONV(out_ch, mid_ch, dirate=1)
        self.pool1 = nn.MaxPool2d(2, stride=2, ceil_mode=True)
        self.rebnconv2 = REBNCONV(mid_ch, mid_ch, dirate=1)
        self.pool2 = nn.MaxPool2d(2, stride=2, ceil_mode=True)
        self.rebnconv3 = REBNCONV(mid_ch, mid_ch, dirate=1)
        self.rebnconv4 = REBNCONV(mid_ch, mid_ch, dirate=2)
        self.rebnconv3d = REBNCONV(mid_ch * 2, mid_ch, dirate=1)
        self.rebnconv2d = REBNCONV(mid_ch * 2, mid_ch, dirate=1)
        self.rebnconv1d = REBNCONV(mid_ch * 2, out_ch, dirate=1)

    def forward(self, x):
        hx = x
        hxin = self.rebnconvin(hx)
        hx1 = self.rebnconv1(hxin)
        hx = self.pool1(hx1)
        hx2 = self.rebnconv2(hx)
        hx = self.pool2(hx2)
        hx3 = self.rebnconv3(hx)
        hx4 = self.rebnconv4(hx3)
        hx3d = self.rebnconv3d(torch.cat((hx4, hx3), 1))
        hx3dup = _upsample_like(hx3d, hx2)
        hx2d = self.rebnconv2d(torch.cat((hx3dup, hx2), 1))
        hx2dup = _upsample_like(hx2d, hx1)
        hx1d = self.rebnconv1d(torch.cat((hx2dup, hx1), 1))
        return hx1d + hxin


class RSU4F(nn.Module):
    """RSU-4F block (no pooling, uses dilated convolutions)."""
    def __init__(self, in_ch=3, mid_ch=12, out_ch=3):
        super(RSU4F, self).__init__()
        self.rebnconvin = REBNCONV(in_ch, out_ch, dirate=1)
        self.rebnconv1 = REBNCONV(out_ch, mid_ch, dirate=1)
        self.rebnconv2 = REBNCONV(mid_ch, mid_ch, dirate=2)
        self.rebnconv3 = REBNCONV(mid_ch, mid_ch, dirate=4)
        self.rebnconv4 = REBNCONV(mid_ch, mid_ch, dirate=8)
        self.rebnconv3d = REBNCONV(mid_ch * 2, mid_ch, dirate=4)
        self.rebnconv2d = REBNCONV(mid_ch * 2, mid_ch, dirate=2)
        self.rebnconv1d = REBNCONV(mid_ch * 2, out_ch, dirate=1)

    def forward(self, x):
        hx = x
        hxin = self.rebnconvin(hx)
        hx1 = self.rebnconv1(hxin)
        hx2 = self.rebnconv2(hx1)
        hx3 = self.rebnconv3(hx2)
        hx4 = self.rebnconv4(hx3)
        hx3d = self.rebnconv3d(torch.cat((hx4, hx3), 1))
        hx2d = self.rebnconv2d(torch.cat((hx3d, hx2), 1))
        hx1d = self.rebnconv1d(torch.cat((hx2d, hx1), 1))
        return hx1d + hxin


class U2NETP(nn.Module):
    """
    U2-Net-P: Lightweight saliency detection network (4.7MB).

    Reference: https://github.com/xuebinqin/U-2-Net
    """
    def __init__(self, in_ch=3, out_ch=1):
        super(U2NETP, self).__init__()

        # Encoder
        self.stage1 = RSU7(in_ch, 16, 64)
        self.pool12 = nn.MaxPool2d(2, stride=2, ceil_mode=True)
        self.stage2 = RSU6(64, 16, 64)
        self.pool23 = nn.MaxPool2d(2, stride=2, ceil_mode=True)
        self.stage3 = RSU5(64, 16, 64)
        self.pool34 = nn.MaxPool2d(2, stride=2, ceil_mode=True)
        self.stage4 = RSU4(64, 16, 64)
        self.pool45 = nn.MaxPool2d(2, stride=2, ceil_mode=True)
        self.stage5 = RSU4F(64, 16, 64)
        self.pool56 = nn.MaxPool2d(2, stride=2, ceil_mode=True)
        self.stage6 = RSU4F(64, 16, 64)

        # Decoder
        self.stage5d = RSU4F(128, 16, 64)
        self.stage4d = RSU4(128, 16, 64)
        self.stage3d = RSU5(128, 16, 64)
        self.stage2d = RSU6(128, 16, 64)
        self.stage1d = RSU7(128, 16, 64)

        # Side outputs
        self.side1 = nn.Conv2d(64, out_ch, 3, padding=1)
        self.side2 = nn.Conv2d(64, out_ch, 3, padding=1)
        self.side3 = nn.Conv2d(64, out_ch, 3, padding=1)
        self.side4 = nn.Conv2d(64, out_ch, 3, padding=1)
        self.side5 = nn.Conv2d(64, out_ch, 3, padding=1)
        self.side6 = nn.Conv2d(64, out_ch, 3, padding=1)

        # Fusion
        self.outconv = nn.Conv2d(6 * out_ch, out_ch, 1)

    def forward(self, x):
        hx = x

        # Encoder
        hx1 = self.stage1(hx)
        hx = self.pool12(hx1)
        hx2 = self.stage2(hx)
        hx = self.pool23(hx2)
        hx3 = self.stage3(hx)
        hx = self.pool34(hx3)
        hx4 = self.stage4(hx)
        hx = self.pool45(hx4)
        hx5 = self.stage5(hx)
        hx = self.pool56(hx5)
        hx6 = self.stage6(hx)
        hx6up = _upsample_like(hx6, hx5)

        # Decoder
        hx5d = self.stage5d(torch.cat((hx6up, hx5), 1))
        hx5dup = _upsample_like(hx5d, hx4)
        hx4d = self.stage4d(torch.cat((hx5dup, hx4), 1))
        hx4dup = _upsample_like(hx4d, hx3)
        hx3d = self.stage3d(torch.cat((hx4dup, hx3), 1))
        hx3dup = _upsample_like(hx3d, hx2)
        hx2d = self.stage2d(torch.cat((hx3dup, hx2), 1))
        hx2dup = _upsample_like(hx2d, hx1)
        hx1d = self.stage1d(torch.cat((hx2dup, hx1), 1))

        # Side outputs
        d1 = self.side1(hx1d)
        d2 = self.side2(hx2d)
        d2 = _upsample_like(d2, d1)
        d3 = self.side3(hx3d)
        d3 = _upsample_like(d3, d1)
        d4 = self.side4(hx4d)
        d4 = _upsample_like(d4, d1)
        d5 = self.side5(hx5d)
        d5 = _upsample_like(d5, d1)
        d6 = self.side6(hx6)
        d6 = _upsample_like(d6, d1)

        # Fusion
        d0 = self.outconv(torch.cat((d1, d2, d3, d4, d5, d6), 1))

        return torch.sigmoid(d0), torch.sigmoid(d1), torch.sigmoid(d2), torch.sigmoid(d3), torch.sigmoid(d4), torch.sigmoid(d5), torch.sigmoid(d6)


class SaliencyDetector:
    """
    Wrapper for U2-Net-P saliency detection.
    Auto-downloads weights on first use.
    """

    def __init__(self, device=None, weights_path='pretrained_models/u2netp.pth'):
        from utils.device import get_best_device
        self.device = device or get_best_device()
        self.weights_path = weights_path
        self.model = None
        self._initialized = False

    def _ensure_initialized(self):
        """Lazy initialization of the model."""
        if self._initialized:
            return

        # Download weights if needed
        self._download_weights()

        # Initialize model
        self.model = U2NETP(3, 1)

        # Load weights
        try:
            state_dict = torch.load(self.weights_path, map_location=self.device, weights_only=True)
            self.model.load_state_dict(state_dict)
            print(f"U2-Net-P saliency model loaded from {self.weights_path}")
        except Exception as e:
            print(f"Warning: Could not load U2-Net-P weights: {e}")
            print("Using randomly initialized saliency model")

        self.model = self.model.to(self.device).eval()
        self._initialized = True

    def ensure_loaded(self):
        """Force eager initialization (call before batch processing)."""
        self._ensure_initialized()

    def _download_weights(self):
        """Download U2-Net-P weights if not present."""
        if os.path.exists(self.weights_path):
            return

        os.makedirs(os.path.dirname(self.weights_path), exist_ok=True)

        print(f"Downloading U2-Net-P weights to {self.weights_path}...")
        try:
            # Try Google Drive direct download
            urllib.request.urlretrieve(U2NETP_GDRIVE_URL, self.weights_path)
            # Check if file is too small (might be HTML error page)
            if os.path.getsize(self.weights_path) < 1000000:  # < 1MB is likely an error
                os.remove(self.weights_path)
                raise Exception("Download resulted in invalid file (too small)")
            print("U2-Net-P download complete.")
        except Exception as e:
            print(f"Failed to auto-download U2-Net-P weights: {e}")
            print("Please download manually from Google Drive:")
            print("  https://drive.google.com/file/d/1rbSTGKAE-MTxBYHd-51l2hMOQPT_7EPy/view")
            print(f"  Save as: {self.weights_path}")
            # Don't raise - will use random initialization

    def detect(self, image_tensor):
        """
        Generate saliency map for input image tensor.

        Args:
            image_tensor: [B, 3, H, W] normalized image tensor

        Returns:
            Saliency map tensor [B, 1, H, W] in range [0, 1]
        """
        self._ensure_initialized()

        with torch.no_grad():
            # U2-Net-P outputs multiple scales, use the fused output (d0)
            d0, *_ = self.model(image_tensor)
            return d0


# =============================================================================
# SAMP-Net Pattern Pooling Module (matches checkpoint structure exactly)
# =============================================================================

class SAMPPModule(nn.Module):
    """
    Saliency-Augmented Multi-pattern Pooling module.

    Implements 8 spatial pooling patterns with architecture matching the
    official CADB checkpoint structure.

    Each pattern has a specific spatial decomposition strategy and
    convolution layer with unique input/output dimensions.
    """

    def __init__(self, in_channels=512, out_channels=1024, num_patterns=8):
        super(SAMPPModule, self).__init__()
        self.num_patterns = num_patterns
        self.in_channels = in_channels
        self.out_channels = out_channels

        # Pattern-specific convolution layers matching checkpoint dimensions exactly
        # Each conv takes concatenated regional features + saliency as input
        # Input channels per pattern (from checkpoint):
        # [1296, 1296, 1373, 1373, 1296, 1296, 1324, 836]
        # Kernel sizes: (2,1), (1,2), (2,1), (2,1), (2,1), (2,2), (2,2), (3,3)
        # Note: bias=False to match checkpoint structure
        self.conv_list = nn.ModuleList([
            nn.Sequential(nn.Conv2d(1296, out_channels, kernel_size=(2, 1), bias=False)),  # Pattern 0
            nn.Sequential(nn.Conv2d(1296, out_channels, kernel_size=(1, 2), bias=False)),  # Pattern 1
            nn.Sequential(nn.Conv2d(1373, out_channels, kernel_size=(2, 1), bias=False)),  # Pattern 2
            nn.Sequential(nn.Conv2d(1373, out_channels, kernel_size=(2, 1), bias=False)),  # Pattern 3
            nn.Sequential(nn.Conv2d(1296, out_channels, kernel_size=(2, 1), bias=False)),  # Pattern 4
            nn.Sequential(nn.Conv2d(1296, out_channels, kernel_size=(2, 2), bias=False)),  # Pattern 5
            nn.Sequential(nn.Conv2d(1324, out_channels, kernel_size=(2, 2), bias=False)),  # Pattern 6
            nn.Sequential(nn.Conv2d(836, out_channels, kernel_size=(3, 3), bias=False)),   # Pattern 7
        ])

    def _get_regional_features(self, feature_map, saliency, pattern_idx, shared_features=None):
        """
        Extract regional features based on pattern spatial decomposition.

        Each pattern requires specific input dimensions matching the checkpoint:
        - Pattern 0: 1296 x 2 x 1 = 2592 total
        - Pattern 1: 1296 x 1 x 2 = 2592 total
        - Pattern 2: 1373 x 2 x 1 = 2746 total
        - Pattern 3: 1373 x 2 x 1 = 2746 total
        - Pattern 4: 1296 x 2 x 1 = 2592 total
        - Pattern 5: 1296 x 2 x 2 = 5184 total
        - Pattern 6: 1324 x 2 x 2 = 5296 total
        - Pattern 7: 836 x 3 x 3 = 7524 total

        Args:
            feature_map: CNN features [B, 512, H, W]
            saliency: Saliency map resized to match feature_map
            pattern_idx: Index of the pattern to compute
            shared_features: Optional dict with precomputed 'global_max', 'global_avg', 'sal_small'
        """
        B, C, H, W = feature_map.shape  # C=512, H=W=7 typically

        # Expected total elements per pattern (from checkpoint conv input shapes)
        expected_totals = [2592, 2592, 2746, 2746, 2592, 5184, 5296, 7524]
        expected_shapes = [(1296, 2, 1), (1296, 1, 2), (1373, 2, 1), (1373, 2, 1),
                          (1296, 2, 1), (1296, 2, 2), (1324, 2, 2), (836, 3, 3)]

        total_needed = expected_totals[pattern_idx]
        c_out, h_out, w_out = expected_shapes[pattern_idx]

        # Use precomputed shared features if provided, otherwise compute
        if shared_features is not None:
            global_max = shared_features['global_max']
            global_avg = shared_features['global_avg']
            sal_small = shared_features['sal_small']
        else:
            # Global pooled features (512 each)
            global_max = F.adaptive_max_pool2d(feature_map, 1).view(B, -1)
            global_avg = F.adaptive_avg_pool2d(feature_map, 1).view(B, -1)
            # Downsample saliency to fixed size for consistent feature count
            sal_small = F.adaptive_avg_pool2d(saliency, (4, 4)).view(B, -1)  # 16 values

        if pattern_idx in [0, 1, 4]:  # 2-region patterns (horizontal, vertical, center)
            # Split into 2 regions
            if pattern_idx == 0:  # Horizontal
                r1 = feature_map[:, :, :H//2, :]
                r2 = feature_map[:, :, H//2:, :]
            elif pattern_idx == 1:  # Vertical
                r1 = feature_map[:, :, :, :W//2]
                r2 = feature_map[:, :, :, W//2:]
            else:  # Center vs surround
                h_m, w_m = H // 4, W // 4
                r1 = feature_map[:, :, h_m:H-h_m, w_m:W-w_m]
                r2 = feature_map

            r1_max = F.adaptive_max_pool2d(r1, 1).view(B, -1)
            r1_avg = F.adaptive_avg_pool2d(r1, 1).view(B, -1)
            r2_max = F.adaptive_max_pool2d(r2, 1).view(B, -1)
            r2_avg = F.adaptive_avg_pool2d(r2, 1).view(B, -1)

            # Base features: 512*4 + 16 = 2064
            feat = torch.cat([r1_max, r1_avg, r2_max, r2_avg, sal_small], dim=1)

        elif pattern_idx in [2, 3]:  # Diagonal patterns (need 2746)
            # More features for diagonal patterns
            r1_max = F.adaptive_max_pool2d(feature_map[:, :, :H//2, :], 1).view(B, -1)
            r1_avg = F.adaptive_avg_pool2d(feature_map[:, :, :H//2, :], 1).view(B, -1)
            r2_max = F.adaptive_max_pool2d(feature_map[:, :, H//2:, :], 1).view(B, -1)
            r2_avg = F.adaptive_avg_pool2d(feature_map[:, :, H//2:, :], 1).view(B, -1)
            center = F.adaptive_max_pool2d(feature_map[:, :, H//4:3*H//4, W//4:3*W//4], 1).view(B, -1)

            # Base features: 512*5 + 16 = 2576
            feat = torch.cat([r1_max, r1_avg, r2_max, r2_avg, center, sal_small], dim=1)

        elif pattern_idx == 5:  # 2x2 quadrant (need 5184)
            q1 = F.adaptive_max_pool2d(feature_map[:, :, :H//2, :W//2], 1).view(B, -1)
            q2 = F.adaptive_max_pool2d(feature_map[:, :, :H//2, W//2:], 1).view(B, -1)
            q3 = F.adaptive_max_pool2d(feature_map[:, :, H//2:, :W//2], 1).view(B, -1)
            q4 = F.adaptive_max_pool2d(feature_map[:, :, H//2:, W//2:], 1).view(B, -1)
            q1a = F.adaptive_avg_pool2d(feature_map[:, :, :H//2, :W//2], 1).view(B, -1)
            q2a = F.adaptive_avg_pool2d(feature_map[:, :, :H//2, W//2:], 1).view(B, -1)
            q3a = F.adaptive_avg_pool2d(feature_map[:, :, H//2:, :W//2], 1).view(B, -1)
            q4a = F.adaptive_avg_pool2d(feature_map[:, :, H//2:, W//2:], 1).view(B, -1)
            center = F.adaptive_max_pool2d(feature_map[:, :, H//4:3*H//4, W//4:3*W//4], 1).view(B, -1)

            # Base features: 512*9 + 16 = 4624
            feat = torch.cat([q1, q2, q3, q4, q1a, q2a, q3a, q4a, center, sal_small], dim=1)

        elif pattern_idx == 6:  # 3x3 grid / rule of thirds (need 5296)
            h3, w3 = H // 3, W // 3
            regions = []
            for i in range(3):
                for j in range(3):
                    r = feature_map[:, :, max(0,i*h3):min(H,(i+1)*h3), max(0,j*w3):min(W,(j+1)*w3)]
                    regions.append(F.adaptive_max_pool2d(r, 1).view(B, -1))

            # Base features: 512*9 + 16 = 4624
            feat = torch.cat(regions + [sal_small], dim=1)

        elif pattern_idx == 7:  # Global pattern (need 7524)
            # More comprehensive features for global pattern
            sal_large = F.adaptive_avg_pool2d(saliency, (8, 8)).view(B, -1)  # 64 values

            # Multiple pooling at different scales
            feats = [global_max, global_avg]
            for scale in [2, 3, 4]:
                pooled = F.adaptive_avg_pool2d(feature_map, scale)
                feats.append(pooled.view(B, -1))

            # Saliency-weighted global
            sal_weight = F.interpolate(saliency, size=(H, W), mode='bilinear', align_corners=False)
            weighted = feature_map * sal_weight
            feats.append(F.adaptive_avg_pool2d(weighted, 1).view(B, -1))

            # Base: 512 + 512 + 512*4 + 512*9 + 512*16 + 512 + 64 = large
            feat = torch.cat(feats + [sal_large], dim=1)

        else:
            feat = torch.cat([global_max, global_avg, sal_small], dim=1)

        # Pad or truncate to exact required size
        current_size = feat.shape[1]
        if current_size < total_needed:
            # Pad with repeated/tiled features
            padding_needed = total_needed - current_size
            # Repeat global features to pad
            pad_feat = global_max.repeat(1, (padding_needed // C) + 1)[:, :padding_needed]
            feat = torch.cat([feat, pad_feat], dim=1)
        elif current_size > total_needed:
            # Truncate
            feat = feat[:, :total_needed]

        # Reshape to expected spatial layout
        return feat.view(B, c_out, h_out, w_out)

    def forward(self, feature_map, saliency_map, pattern_weights):
        """
        Apply multi-pattern pooling weighted by pattern weights.

        Args:
            feature_map: CNN features [B, 512, H, W]
            saliency_map: Downsampled saliency [B, 1, H', W']
            pattern_weights: Learned pattern weights [B, num_patterns]

        Returns:
            Tuple of (aggregated features [B, out_channels], pattern features [B, num_patterns, out_channels])
        """
        B = feature_map.shape[0]

        # Resize saliency to match feature map
        saliency_resized = F.interpolate(
            saliency_map,
            size=feature_map.shape[2:],
            mode='bilinear',
            align_corners=False
        )

        # Precompute shared features once (15-25% speedup)
        shared_features = {
            'global_max': F.adaptive_max_pool2d(feature_map, 1).view(B, -1),
            'global_avg': F.adaptive_avg_pool2d(feature_map, 1).view(B, -1),
            'sal_small': F.adaptive_avg_pool2d(saliency_resized, (4, 4)).view(B, -1),
        }

        # Collect pattern features
        pattern_features = []
        for i in range(self.num_patterns):
            # Get regional features reshaped for this pattern's conv (pass precomputed features)
            regional_feat = self._get_regional_features(feature_map, saliency_resized, i, shared_features)
            # Apply pattern-specific convolution
            conv_out = self.conv_list[i](regional_feat)  # [B, out_channels, 1, 1]
            pattern_features.append(conv_out.view(B, -1))

        # Stack: [B, num_patterns, out_channels]
        pattern_features = torch.stack(pattern_features, dim=1)

        # Weight and aggregate
        weights = F.softmax(pattern_weights, dim=1).unsqueeze(2)

        # Weighted sum: [B, out_channels]
        aggregated = (pattern_features * weights).sum(dim=1)

        return aggregated, pattern_features


# =============================================================================
# SAMP-Net Main Architecture (matches checkpoint structure)
# =============================================================================

def build_resnet18_backbone(pretrained=False):
    """
    Build ResNet-18 backbone matching the checkpoint structure.

    The checkpoint uses: nn.Sequential(*list(resnet.children())[:-2])
    This removes avgpool and fc, keeping conv layers as backbone.0, backbone.1, etc.
    """
    resnet = models.resnet18(weights=models.ResNet18_Weights.DEFAULT if pretrained else None)
    # Remove avgpool and fc layers
    modules = list(resnet.children())[:-2]
    return nn.Sequential(*modules)


class SAMPNet(nn.Module):
    """
    SAMP-Net: Saliency-Augmented Multi-pattern Pooling Network.

    Architecture matches the official CADB checkpoint structure exactly:
    - ResNet-18 backbone (children[:-2] as Sequential)
    - Saliency map downsampling
    - SAMPPModule for pattern-based pooling (outputs 1024-dim features)
    - Pattern weight prediction layer: Linear(512, 8)
    - Attribute prediction: Linear(1024, 512) -> Linear(512, 6)
    - Composition score: Linear(1024, 1024) -> Linear(1024, 512) -> Linear(512, 5)

    Reference: https://github.com/bcmi/Image-Composition-Assessment-Dataset-CADB/tree/main/SAMPNet
    """

    def __init__(self, num_patterns=8, num_attributes=6, num_scores=5):
        super(SAMPNet, self).__init__()

        self.num_patterns = num_patterns
        self.num_attributes = num_attributes
        self.num_scores = num_scores

        # Backbone: ResNet-18 without avgpool/fc
        # Output: [B, 512, 7, 7] for 224x224 input
        self.backbone = build_resnet18_backbone(pretrained=False)

        # Saliency map downsampling (matches feature map spatial size)
        self.saliency_max = nn.Sequential(
            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),
            nn.MaxPool2d(kernel_size=3, stride=2, padding=1)
        )

        # Pattern weight prediction: avgpool -> flatten -> Linear(512, 8)
        # Checkpoint shows pattern_weight_layer.3.weight: (8, 512)
        # Note: bias=False to match checkpoint
        self.pattern_weight_layer = nn.Sequential(
            nn.AdaptiveAvgPool2d(1),  # 0
            nn.Flatten(),              # 1
            nn.ReLU(inplace=True),     # 2
            nn.Linear(512, num_patterns, bias=False)  # 3
        )

        # Pattern pooling module (outputs 1024-dim features)
        self.pattern_module = SAMPPModule(
            in_channels=512,
            out_channels=1024,
            num_patterns=num_patterns
        )

        # Attribute prediction head
        # Checkpoint: att_feature_layer.0.weight: (512, 1024)
        #             att_pred_layer.0.weight: (6, 512)
        # Note: bias=False to match checkpoint
        self.att_feature_layer = nn.Sequential(
            nn.Linear(1024, 512, bias=False),
            nn.ReLU(inplace=True),
            nn.Dropout(0.5)
        )
        self.att_pred_layer = nn.Sequential(
            nn.Linear(512, num_attributes, bias=False),
            nn.Sigmoid()
        )

        # Composition feature layer
        # Checkpoint: com_feature_layer.0.weight: (512, 1024)
        # Note: bias=False to match checkpoint
        self.com_feature_layer = nn.Sequential(
            nn.Linear(1024, 512, bias=False),
            nn.ReLU(inplace=True),
            nn.Dropout(0.5)
        )

        # Alpha prediction layer (outputs 2 values)
        # Checkpoint: alpha_predict_layer.0.weight: (2, 1024)
        # Note: bias=False to match checkpoint
        self.alpha_predict_layer = nn.Sequential(
            nn.Linear(1024, 2, bias=False),
            nn.Sigmoid()
        )

        # Final score distribution head
        # Checkpoint: com_pred_layer.0.weight: (1024, 1024)
        #             com_pred_layer.3.weight: (512, 1024)
        #             com_pred_layer.5.weight: (5, 512)
        # Note: bias=False to match checkpoint
        self.com_pred_layer = nn.Sequential(
            nn.Linear(1024, 1024, bias=False),     # 0
            nn.ReLU(inplace=True),                 # 1
            nn.Dropout(0.5),                       # 2
            nn.Linear(1024, 512, bias=False),      # 3
            nn.ReLU(inplace=True),                 # 4
            nn.Linear(512, num_scores, bias=False),  # 5
            nn.Softmax(dim=1)
        )

    def forward(self, x, saliency):
        """
        Forward pass with image and saliency map.

        Args:
            x: Image tensor [B, 3, 224, 224]
            saliency: Saliency map [B, 1, 224, 224]

        Returns:
            Tuple of (pattern_weights, attributes, score_distribution)
        """
        # Extract backbone features
        feature_map = self.backbone(x)  # [B, 512, 7, 7]

        # Predict pattern weights from features
        pattern_weights = self.pattern_weight_layer(feature_map)  # [B, num_patterns]

        # Downsample saliency to match feature spatial size
        sal_down = self.saliency_max(saliency)  # [B, 1, ~56, ~56]

        # Pattern-based pooling (outputs 1024-dim features)
        pattern_feat, all_pattern_feats = self.pattern_module(feature_map, sal_down, pattern_weights)

        # Attribute prediction
        att_feat = self.att_feature_layer(pattern_feat)
        attributes = self.att_pred_layer(att_feat)

        # Composition score prediction
        # Note: com_pred_layer takes the raw pattern_feat (1024-dim), not com_feature_layer output
        score_dist = self.com_pred_layer(pattern_feat)

        return pattern_weights, attributes, score_dist


# =============================================================================
# High-level Scorer API
# =============================================================================

class SAMPNetScorer:
    """
    High-level wrapper for SAMP-Net composition scoring.

    Automatically generates saliency maps using U2-Net-P and feeds them
    to SAMP-Net for composition assessment.

    Usage:
        scorer = SAMPNetScorer()
        result = scorer.score('image.jpg')
        # result: {'comp_score': 7.5, 'pattern': 'rule_of_thirds', ...}
    """

    def __init__(self, model_path: str = None, device: str = None):
        """
        Initialize SAMP-Net scorer with saliency detector.

        Args:
            model_path: Path to SAMP-Net weights. If None, uses default.
            device: Target device. If None, auto-detects best available.
        """
        from utils.device import get_best_device
        self.device = device or get_best_device()
        self.model_path = model_path or 'pretrained_models/samp_net.pth'

        # Image preprocessing (224x224 for SAMP-Net)
        self.transform = transforms.Compose([
            transforms.Resize((224, 224)),
            transforms.ToTensor(),
            transforms.Normalize(
                mean=[0.485, 0.456, 0.406],
                std=[0.229, 0.224, 0.225]
            )
        ])

        # Initialize saliency detector (lazy loading)
        self.saliency_detector = SaliencyDetector(
            device=self.device,
            weights_path='pretrained_models/u2netp.pth'
        )

        # Load SAMP-Net model
        self.model = self._load_model()

    def ensure_loaded(self):
        """Force eager loading of saliency model (call before batch processing)."""
        self.saliency_detector.ensure_loaded()

    def _download_weights(self):
        """Download SAMP-Net weights if not present."""
        if os.path.exists(self.model_path):
            return

        os.makedirs(os.path.dirname(self.model_path), exist_ok=True)

        print(f"Downloading SAMP-Net weights to {self.model_path}...")
        try:
            urllib.request.urlretrieve(SAMPNET_WEIGHTS_URL, self.model_path)
            print("SAMP-Net download complete.")
        except Exception as e:
            print(f"Failed to download SAMP-Net weights: {e}")
            print("Please download manually from Dropbox:")
            print("  https://www.dropbox.com/scl/fi/k1yuyhotuk9ky3m41iobg/samp_net.pth?rlkey=aoqqxv27wd5qqj3pytxki6vi3&dl=1")
            print(f"  Place at: {self.model_path}")
            raise

    def _load_model(self):
        """Load and initialize the SAMP-Net model."""
        self._download_weights()

        # Initialize model architecture
        model = SAMPNet(num_patterns=8, num_attributes=6, num_scores=5)

        # Load checkpoint
        try:
            print(f"Loading SAMP-Net weights from {self.model_path}...")
            checkpoint = torch.load(self.model_path, map_location=self.device, weights_only=False)

            # Handle different checkpoint formats
            if 'model_state_dict' in checkpoint:
                state_dict = checkpoint['model_state_dict']
            elif 'state_dict' in checkpoint:
                state_dict = checkpoint['state_dict']
            else:
                state_dict = checkpoint

            # Check for key mismatches (for debugging)
            model_keys = set(model.state_dict().keys())
            ckpt_keys = set(state_dict.keys())
            missing = model_keys - ckpt_keys
            unexpected = ckpt_keys - model_keys

            if missing:
                print(f"[SAMP-Net] Note: {len(missing)} model keys not in checkpoint")
            if unexpected:
                print(f"[SAMP-Net] Note: {len(unexpected)} checkpoint keys not in model")

            # Load with strict=False to handle any mismatches gracefully
            model.load_state_dict(state_dict, strict=False)
            print("SAMP-Net model loaded successfully")

        except Exception as e:
            print(f"Warning: Could not load SAMP-Net weights: {e}")
            print("Using randomly initialized model (scores may not be accurate)")

        return model.to(self.device).eval()

    def preprocess(self, image):
        """
        Preprocess image for model input.

        Args:
            image: PIL Image, numpy array (BGR), or file path

        Returns:
            Tensor [1, 3, 224, 224] ready for model
        """
        if isinstance(image, str):
            image = Image.open(image).convert('RGB')
        elif isinstance(image, np.ndarray):
            # Assume BGR (OpenCV) -> RGB
            if len(image.shape) == 3 and image.shape[2] == 3:
                import cv2
                image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
            image = Image.fromarray(image)
        elif not isinstance(image, Image.Image):
            raise ValueError(f"Unsupported image type: {type(image)}")

        if image.mode != 'RGB':
            image = image.convert('RGB')

        return self.transform(image).unsqueeze(0).to(self.device)

    def score(self, image) -> dict:
        """
        Score image composition using SAMP-Net with saliency.

        Args:
            image: PIL Image, numpy array (BGR), or file path

        Returns:
            dict with:
                - 'comp_score': float (0-10 scale)
                - 'raw_score': float (1-5 scale)
                - 'pattern': str (dominant composition pattern)
                - 'pattern_index': int (pattern index 0-7)
                - 'pattern_weights': dict (weight per pattern)
                - 'score_distribution': list (5-class probabilities)
                - 'attributes': list (6 attribute predictions)
        """
        # Preprocess image
        input_tensor = self.preprocess(image)

        # Generate saliency map
        saliency = self.saliency_detector.detect(input_tensor)

        # Run SAMP-Net inference
        with torch.no_grad():
            pattern_weights, attributes, score_dist = self.model(input_tensor, saliency)

        # Process outputs
        pattern_weights_np = F.softmax(pattern_weights[0], dim=0).cpu().numpy()
        attributes_np = attributes[0].cpu().numpy()
        score_dist_np = score_dist[0].cpu().numpy()

        # Find dominant pattern
        dominant_idx = int(np.argmax(pattern_weights_np))

        # Calculate expected score (1-5 scale)
        scores = np.array([1, 2, 3, 4, 5])
        raw_score = float(np.sum(scores * score_dist_np))

        # Normalize to 0-10 scale
        comp_score = (raw_score - 1) / 4.0 * 10.0
        comp_score = max(0.0, min(10.0, comp_score))

        # Build pattern weights dict
        pattern_weights_dict = {
            COMPOSITION_PATTERNS[i]: float(pattern_weights_np[i])
            for i in range(len(COMPOSITION_PATTERNS))
        }

        return {
            'comp_score': round(comp_score, 2),
            'raw_score': round(raw_score, 2),
            'pattern': COMPOSITION_PATTERNS[dominant_idx],
            'pattern_index': dominant_idx,
            'pattern_weights': pattern_weights_dict,
            'score_distribution': score_dist_np.tolist(),
            'attributes': attributes_np.tolist(),
            # Legacy compatibility
            'power_point_score': round(comp_score / 2, 2),
        }

    def score_batch(self, images: list) -> list:
        """
        Score a batch of images.

        Args:
            images: List of PIL Images, numpy arrays, or file paths

        Returns:
            List of score dicts
        """
        # Preprocess all images
        tensors = [self.preprocess(img) for img in images]
        batch_tensor = torch.cat(tensors, dim=0)

        # Generate saliency maps for batch
        saliency_batch = self.saliency_detector.detect(batch_tensor)

        # Batch inference
        with torch.no_grad():
            pattern_weights, attributes, score_dist = self.model(batch_tensor, saliency_batch)

        # Process each result
        results = []
        batch_size = batch_tensor.size(0)

        for i in range(batch_size):
            pw_np = F.softmax(pattern_weights[i], dim=0).cpu().numpy()
            attr_np = attributes[i].cpu().numpy()
            sd_np = score_dist[i].cpu().numpy()

            dominant_idx = int(np.argmax(pw_np))
            scores = np.array([1, 2, 3, 4, 5])
            raw_score = float(np.sum(scores * sd_np))
            comp_score = (raw_score - 1) / 4.0 * 10.0
            comp_score = max(0.0, min(10.0, comp_score))

            pattern_weights_dict = {
                COMPOSITION_PATTERNS[j]: float(pw_np[j])
                for j in range(len(COMPOSITION_PATTERNS))
            }

            results.append({
                'comp_score': round(comp_score, 2),
                'raw_score': round(raw_score, 2),
                'pattern': COMPOSITION_PATTERNS[dominant_idx],
                'pattern_index': dominant_idx,
                'pattern_weights': pattern_weights_dict,
                'score_distribution': sd_np.tolist(),
                'attributes': attr_np.tolist(),
                'power_point_score': round(comp_score / 2, 2),
            })

        return results


def create_samp_scorer(config) -> SAMPNetScorer:
    """
    Factory function to create SAMP-Net scorer from config.

    Args:
        config: ScoringConfig or dict with samp_net settings

    Returns:
        SAMPNetScorer instance or None if initialization fails
    """
    try:
        if hasattr(config, 'get_samp_net_config'):
            samp_config = config.get_samp_net_config()
        elif isinstance(config, dict):
            samp_config = config.get('samp_net', {})
        else:
            samp_config = {}

        model_path = samp_config.get('model_path', 'pretrained_models/samp_net.pth')
        from utils.device import get_best_device
        device = get_best_device()

        return SAMPNetScorer(model_path=model_path, device=device)
    except Exception as e:
        print(f"Failed to initialize SAMP-Net scorer: {e}")
        return None
